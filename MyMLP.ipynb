{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Binary Cross Entropy Loss\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Xavier initialization\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Output layer error\n",
        "        d_a2 = output - y\n",
        "        d_z2 = d_a2 * sigmoid_derivative(output)\n",
        "        d_W2 = np.dot(self.a1.T, d_z2) / m\n",
        "        d_b2 = np.sum(d_z2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Hidden layer error\n",
        "        d_a1 = np.dot(d_z2, self.W2.T)\n",
        "        d_z1 = d_a1 * sigmoid_derivative(self.a1)\n",
        "        d_W1 = np.dot(X.T, d_z1) / m\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights\n",
        "        self.W2 -= learning_rate * d_W2\n",
        "        self.b2 -= learning_rate * d_b2\n",
        "        self.W1 -= learning_rate * d_W1\n",
        "        self.b1 -= learning_rate * d_b1\n",
        "\n",
        "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = binary_cross_entropy(y, output)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # XOR dataset\n",
        "    X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "    y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "    mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
        "    mlp.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "    predictions = mlp.predict(X)\n",
        "    print(\"Predictions:\\n\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0h3_fn273u2",
        "outputId": "1c21916b-a617-425a-a10c-b5e6bdf23689"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7593\n",
            "Epoch 100, Loss: 0.6978\n",
            "Epoch 200, Loss: 0.6933\n",
            "Epoch 300, Loss: 0.6930\n",
            "Epoch 400, Loss: 0.6929\n",
            "Epoch 500, Loss: 0.6928\n",
            "Epoch 600, Loss: 0.6927\n",
            "Epoch 700, Loss: 0.6926\n",
            "Epoch 800, Loss: 0.6925\n",
            "Epoch 900, Loss: 0.6924\n",
            "Epoch 1000, Loss: 0.6923\n",
            "Epoch 1100, Loss: 0.6922\n",
            "Epoch 1200, Loss: 0.6921\n",
            "Epoch 1300, Loss: 0.6920\n",
            "Epoch 1400, Loss: 0.6919\n",
            "Epoch 1500, Loss: 0.6917\n",
            "Epoch 1600, Loss: 0.6916\n",
            "Epoch 1700, Loss: 0.6915\n",
            "Epoch 1800, Loss: 0.6914\n",
            "Epoch 1900, Loss: 0.6912\n",
            "Epoch 2000, Loss: 0.6911\n",
            "Epoch 2100, Loss: 0.6909\n",
            "Epoch 2200, Loss: 0.6908\n",
            "Epoch 2300, Loss: 0.6906\n",
            "Epoch 2400, Loss: 0.6904\n",
            "Epoch 2500, Loss: 0.6902\n",
            "Epoch 2600, Loss: 0.6900\n",
            "Epoch 2700, Loss: 0.6898\n",
            "Epoch 2800, Loss: 0.6896\n",
            "Epoch 2900, Loss: 0.6893\n",
            "Epoch 3000, Loss: 0.6891\n",
            "Epoch 3100, Loss: 0.6888\n",
            "Epoch 3200, Loss: 0.6885\n",
            "Epoch 3300, Loss: 0.6882\n",
            "Epoch 3400, Loss: 0.6879\n",
            "Epoch 3500, Loss: 0.6875\n",
            "Epoch 3600, Loss: 0.6871\n",
            "Epoch 3700, Loss: 0.6867\n",
            "Epoch 3800, Loss: 0.6863\n",
            "Epoch 3900, Loss: 0.6858\n",
            "Epoch 4000, Loss: 0.6854\n",
            "Epoch 4100, Loss: 0.6848\n",
            "Epoch 4200, Loss: 0.6843\n",
            "Epoch 4300, Loss: 0.6837\n",
            "Epoch 4400, Loss: 0.6831\n",
            "Epoch 4500, Loss: 0.6824\n",
            "Epoch 4600, Loss: 0.6817\n",
            "Epoch 4700, Loss: 0.6810\n",
            "Epoch 4800, Loss: 0.6802\n",
            "Epoch 4900, Loss: 0.6794\n",
            "Epoch 5000, Loss: 0.6785\n",
            "Epoch 5100, Loss: 0.6775\n",
            "Epoch 5200, Loss: 0.6766\n",
            "Epoch 5300, Loss: 0.6755\n",
            "Epoch 5400, Loss: 0.6744\n",
            "Epoch 5500, Loss: 0.6733\n",
            "Epoch 5600, Loss: 0.6721\n",
            "Epoch 5700, Loss: 0.6708\n",
            "Epoch 5800, Loss: 0.6694\n",
            "Epoch 5900, Loss: 0.6680\n",
            "Epoch 6000, Loss: 0.6666\n",
            "Epoch 6100, Loss: 0.6650\n",
            "Epoch 6200, Loss: 0.6634\n",
            "Epoch 6300, Loss: 0.6618\n",
            "Epoch 6400, Loss: 0.6600\n",
            "Epoch 6500, Loss: 0.6582\n",
            "Epoch 6600, Loss: 0.6564\n",
            "Epoch 6700, Loss: 0.6544\n",
            "Epoch 6800, Loss: 0.6524\n",
            "Epoch 6900, Loss: 0.6503\n",
            "Epoch 7000, Loss: 0.6481\n",
            "Epoch 7100, Loss: 0.6459\n",
            "Epoch 7200, Loss: 0.6436\n",
            "Epoch 7300, Loss: 0.6413\n",
            "Epoch 7400, Loss: 0.6388\n",
            "Epoch 7500, Loss: 0.6363\n",
            "Epoch 7600, Loss: 0.6338\n",
            "Epoch 7700, Loss: 0.6312\n",
            "Epoch 7800, Loss: 0.6285\n",
            "Epoch 7900, Loss: 0.6258\n",
            "Epoch 8000, Loss: 0.6231\n",
            "Epoch 8100, Loss: 0.6203\n",
            "Epoch 8200, Loss: 0.6175\n",
            "Epoch 8300, Loss: 0.6146\n",
            "Epoch 8400, Loss: 0.6117\n",
            "Epoch 8500, Loss: 0.6088\n",
            "Epoch 8600, Loss: 0.6058\n",
            "Epoch 8700, Loss: 0.6028\n",
            "Epoch 8800, Loss: 0.5998\n",
            "Epoch 8900, Loss: 0.5968\n",
            "Epoch 9000, Loss: 0.5938\n",
            "Epoch 9100, Loss: 0.5908\n",
            "Epoch 9200, Loss: 0.5877\n",
            "Epoch 9300, Loss: 0.5847\n",
            "Epoch 9400, Loss: 0.5816\n",
            "Epoch 9500, Loss: 0.5785\n",
            "Epoch 9600, Loss: 0.5755\n",
            "Epoch 9700, Loss: 0.5724\n",
            "Epoch 9800, Loss: 0.5693\n",
            "Epoch 9900, Loss: 0.5662\n",
            "Predictions:\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Use only two classes: 0 (setosa) and 1 (versicolor)\n",
        "binary_filter = y < 2\n",
        "X = X[binary_filter]\n",
        "y = y[binary_filter].reshape(-1, 1)  # Make it a column vector\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "laP1SYYj-You"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP(input_size=4, hidden_size=6, output_size=1)\n",
        "mlp.train(X_train, y_train, epochs=5000, learning_rate=0.1)\n",
        "predictions = mlp.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(\"Test Accuracy:\", accuracy * 100, \"%\")\n",
        "print(\"Predictions:\\n\", predictions)\n",
        "print(\"True Labels:\\n\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWOn8ri8-agS",
        "outputId": "5e5cea8b-1877-4284-a635-732ce06558ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7341\n",
            "Epoch 100, Loss: 0.6442\n",
            "Epoch 200, Loss: 0.6300\n",
            "Epoch 300, Loss: 0.6153\n",
            "Epoch 400, Loss: 0.5982\n",
            "Epoch 500, Loss: 0.5784\n",
            "Epoch 600, Loss: 0.5559\n",
            "Epoch 700, Loss: 0.5307\n",
            "Epoch 800, Loss: 0.5031\n",
            "Epoch 900, Loss: 0.4735\n",
            "Epoch 1000, Loss: 0.4428\n",
            "Epoch 1100, Loss: 0.4119\n",
            "Epoch 1200, Loss: 0.3816\n",
            "Epoch 1300, Loss: 0.3527\n",
            "Epoch 1400, Loss: 0.3258\n",
            "Epoch 1500, Loss: 0.3011\n",
            "Epoch 1600, Loss: 0.2787\n",
            "Epoch 1700, Loss: 0.2587\n",
            "Epoch 1800, Loss: 0.2408\n",
            "Epoch 1900, Loss: 0.2248\n",
            "Epoch 2000, Loss: 0.2106\n",
            "Epoch 2100, Loss: 0.1980\n",
            "Epoch 2200, Loss: 0.1867\n",
            "Epoch 2300, Loss: 0.1766\n",
            "Epoch 2400, Loss: 0.1676\n",
            "Epoch 2500, Loss: 0.1594\n",
            "Epoch 2600, Loss: 0.1520\n",
            "Epoch 2700, Loss: 0.1453\n",
            "Epoch 2800, Loss: 0.1392\n",
            "Epoch 2900, Loss: 0.1336\n",
            "Epoch 3000, Loss: 0.1285\n",
            "Epoch 3100, Loss: 0.1238\n",
            "Epoch 3200, Loss: 0.1195\n",
            "Epoch 3300, Loss: 0.1155\n",
            "Epoch 3400, Loss: 0.1118\n",
            "Epoch 3500, Loss: 0.1083\n",
            "Epoch 3600, Loss: 0.1051\n",
            "Epoch 3700, Loss: 0.1021\n",
            "Epoch 3800, Loss: 0.0993\n",
            "Epoch 3900, Loss: 0.0966\n",
            "Epoch 4000, Loss: 0.0941\n",
            "Epoch 4100, Loss: 0.0918\n",
            "Epoch 4200, Loss: 0.0896\n",
            "Epoch 4300, Loss: 0.0875\n",
            "Epoch 4400, Loss: 0.0855\n",
            "Epoch 4500, Loss: 0.0836\n",
            "Epoch 4600, Loss: 0.0819\n",
            "Epoch 4700, Loss: 0.0802\n",
            "Epoch 4800, Loss: 0.0785\n",
            "Epoch 4900, Loss: 0.0770\n",
            "Test Accuracy: 100.0 %\n",
            "Predictions:\n",
            " [[1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "True Labels:\n",
            " [[1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    }
  ]
}